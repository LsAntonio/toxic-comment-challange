{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model](model_notebook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data(label):\n",
    "    '''Return the train data based in the label\n",
    "    col index:\n",
    "        0: id\n",
    "        1: comment_text\n",
    "        2: toxic\n",
    "        3: severe_toxic\n",
    "        4: obscene\n",
    "        5: threat\n",
    "        6: insult\n",
    "        7: identity_hate\n",
    "    label:\n",
    "        - toxic\n",
    "        - severe_toxic\n",
    "        - obscene\n",
    "        - threat\n",
    "        - insult\n",
    "        - identity_hate\n",
    "    '''\n",
    "    if label == 'toxic':\n",
    "        data = pd.read_csv(\"../data/train.csv\", sep = \",\", dtype = {'comment_text': str, 'toxic': int}, usecols = [1, 2])\n",
    "    elif label == 'severe_toxic':\n",
    "        data = pd.read_csv(\"../data/train.csv\", sep = \",\", dtype = {'comment_text': str, 'severe_toxic': int}, usecols = [1, 3])\n",
    "    elif label == 'obscene':\n",
    "        data = pd.read_csv(\"../data/train.csv\", sep = \",\", dtype = {'comment_text': str, 'obscene': int}, usecols = [1, 4])\n",
    "    elif label == 'threat':\n",
    "        data = pd.read_csv(\"../data/train.csv\", sep = \",\", dtype = {'comment_text': str, 'threat': int}, usecols = [1, 5])\n",
    "    elif label == 'insult':\n",
    "        data = pd.read_csv(\"../data/train.csv\", sep = \",\", dtype = {'comment_text': str, 'insult': int}, usecols = [1, 6])\n",
    "    elif label == 'identity_hate':\n",
    "        data = pd.read_csv(\"../data/train.csv\", sep = \",\", dtype = {'comment_text': str, 'identity_hate': int}, usecols = [1, 7])\n",
    "    else:\n",
    "        print('Not defined!...')\n",
    "    return data\n",
    "\n",
    "def read_test_data():\n",
    "    '''Return the test data\n",
    "    0: ids\n",
    "    1: comment_text\n",
    "    '''\n",
    "    return pd.read_csv(\"../data/test.csv\", sep = \",\", dtype = {'id': str, 'comment_text': str})\n",
    "\n",
    "def pre_processing(data):\n",
    "    '''\n",
    "    Pre process the data\n",
    "    data: data set to pre process\n",
    "    '''\n",
    "    # to lower\n",
    "    data['comment_text'].fillna('unknow', inplace = True)\n",
    "    data['comment_text'] = data['comment_text'].str.lower()\n",
    "    # replace don't = n't = [space] not => do not\n",
    "    data.comment_text.replace(\"n't\", value = ' not', inplace = True, regex = True)\n",
    "    # 'll will\n",
    "    data.comment_text.replace(\"'ll\", value = ' will', inplace = True, regex = True)\n",
    "    # remove \\n, dots, commas\n",
    "    data.comment_text.replace('[^a-zA-Z]+', value = ' ', inplace = True, regex = True)\n",
    "    return data\n",
    "    \n",
    "def generate_features(data, label, num_features):\n",
    "    '''This function creates TfidfVectorizer features\n",
    "    data: data set\n",
    "    label:\n",
    "        - toxic\n",
    "        - severe_toxic\n",
    "        - obscene\n",
    "        - threat\n",
    "        - insult\n",
    "        - identity_hate\n",
    "    num_features: Number of features to generate\n",
    "    '''\n",
    "    tiff = TfidfVectorizer(max_df = 0.95, min_df = 2, max_features = num_features, stop_words = 'english')\n",
    "    feature_tiff = tiff.fit_transform(data['comment_text'])\n",
    "    y = data[label].values\n",
    "    del data\n",
    "    gc.collect()\n",
    "    return feature_tiff, y, tiff\n",
    "\n",
    "\n",
    "def split_data(data, y, random_seed):\n",
    "    '''Split the data set\n",
    "    data: data set\n",
    "    y: labels\n",
    "    random_seed: for reproducibility\n",
    "    '''\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(data, y, stratify = y, random_state = random_seed, test_size = .3)\n",
    "    X_dev, X_test, y_dev, y_test = train_test_split(X_dev, y_dev, stratify = y_dev, random_state = random_seed, test_size = .5)\n",
    "    \n",
    "    del data, y # 159571 samples\n",
    "    gc.collect()\n",
    "    \n",
    "    return X_train, X_dev, X_test, y_train, y_dev, y_test\n",
    "\n",
    "def get_imb_data_info(label):\n",
    "    '''Show the classes in the data and return the ratio to use in\n",
    "    catboost hiperparameter scale_pos_weight\n",
    "    label:\n",
    "        - toxic\n",
    "        - severe_toxic\n",
    "        - obscene\n",
    "        - threat\n",
    "        - insult\n",
    "        - identity_hate\n",
    "    '''\n",
    "    data = read_train_data(label)\n",
    "    count_classes = pd.DataFrame(data[label].value_counts().index, columns = [label])\n",
    "    count_classes['count'] = list(data[label].value_counts())\n",
    "    del data\n",
    "    gc.collect()\n",
    "    print(count_classes)\n",
    "    ratio = count_classes['count'][0] / count_classes['count'][1]\n",
    "    print('Neg examples {} / Pos examples {} = {}'.format(count_classes['count'][0], count_classes['count'][1], ratio))\n",
    "    return ratio\n",
    "\n",
    "def individual_predictions(test, model):\n",
    "    '''Make predictions using a model\n",
    "    model: Dictionary containing the models and TfidfVectorizer transformer, ex model['insult']\n",
    "    test: data set used for test\n",
    "    '''\n",
    "    test = model['tiff'].transform(test['comment_text']) # transform test\n",
    "    \n",
    "    temp = pd.DataFrame(model['BernoulliNB_1'].predict_proba(test)[:, 0], columns = ['BernoulliNB_1_0'])\n",
    "    temp['BernoulliNB_1_1'] = model['BernoulliNB_1'].predict_proba(test)[:, 1]\n",
    "    temp['RidgeClassifier'] = model['RidgeClassifier'].predict(test)\n",
    "    temp['AdaBoostClassifier_0'] = model['AdaBoostClassifier'].predict_proba(test)[:, 0]\n",
    "    temp['AdaBoostClassifier_1'] = model['AdaBoostClassifier'].predict_proba(test)[:, 1]\n",
    "    temp['BernoulliNB_2_0'] = model['BernoulliNB_2'].predict_proba(test)[:, 0]\n",
    "    temp['BernoulliNB_2_1'] = model['BernoulliNB_2'].predict_proba(test)[:, 1]\n",
    "        \n",
    "    return model['CatBoostClassifier'].predict_proba(temp)[:, 1]\n",
    "    \n",
    "    \n",
    "def predict(models):\n",
    "    '''This function predict over the test data and return the submission file\n",
    "    models: A dictionary containing the models and TfidfVectorizer transformer.\n",
    "        - models[0]: toxic\n",
    "        - models[1]: severe_toxic\n",
    "        - models[2]: obscene\n",
    "        - models[3]: threat\n",
    "        - models[4]: insult\n",
    "        - models[5]: identity_hate\n",
    "    '''\n",
    "    print('Loading test data...')\n",
    "    test = read_test_data()\n",
    "\n",
    "    print('Pre processing...')\n",
    "    test = pre_processing(test)\n",
    "    \n",
    "    print('Predictions')\n",
    "    submit = pd.DataFrame(test['id'].values, columns = ['id'])\n",
    "\n",
    "    submit['toxic'] = individual_predictions(test, models['toxic'])\n",
    "    submit['severe_toxic'] = individual_predictions(test, models['severe_toxic'])\n",
    "    submit['obscene'] = individual_predictions(test, models['obscene'])\n",
    "    submit['threat'] = individual_predictions(test, models['threat'])\n",
    "    submit['insult'] = individual_predictions(test, models['insult'])\n",
    "    submit['identity_hate'] = individual_predictions(test, models['identity_hate'])\n",
    "    \n",
    "    return submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(label, random_seed, cat_parameters):\n",
    "    '''Train all the models for a specific class\n",
    "    label:\n",
    "        - toxic\n",
    "        - severe_toxic\n",
    "        - obscene\n",
    "        - threat\n",
    "        - insult\n",
    "        - identity_hate\n",
    "    random_seed: For reproducibility\n",
    "    cat_parameters: catBoost hyperparameters\n",
    "    '''\n",
    "    data = read_train_data(label)\n",
    "    data = pre_processing(data)\n",
    "    \n",
    "    feature_tiff, y, tiff = generate_features(data, label, 5000)\n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = split_data(feature_tiff, y, random_seed)\n",
    "    X = [(X_train, y_train), (X_dev, y_dev), (X_test, y_test)]\n",
    "    \n",
    "    print('BernoulliNB [1]...')\n",
    "    berNB_m1 = BernoulliNB(alpha = 1.0)\n",
    "    berNB_m1.fit(X_train, y_train)\n",
    "    get_results('BernoulliNB_1', berNB_m1, X)\n",
    "    \n",
    "    print('RidgeClassifier...')\n",
    "    ridgeC = RidgeClassifier(normalize = True, random_state = 7)\n",
    "    ridgeC.fit(X_train, y_train)\n",
    "    get_results('RidgeClassifier', ridgeC, X)\n",
    "    \n",
    "    print('Ada Boost...')\n",
    "    ada = AdaBoostClassifier(random_state = 7)\n",
    "    ada.fit(X_train, y_train)\n",
    "    get_results('AdaBoostClassifier', ada, X)\n",
    "    \n",
    "    print('BernoulliNB [2]...')\n",
    "    berNB_m2 = BernoulliNB(alpha = 0.5)\n",
    "    berNB_m2.fit(X_train, y_train)\n",
    "    get_results('BernoulliNB_2', berNB_m2, X)\n",
    "       \n",
    "    print('Creating temp train/dev/test data...')\n",
    "\n",
    "    temp_train = pd.DataFrame(berNB_m1.predict_proba(X_train)[:, 0], columns = ['BernoulliNB_1_0'])\n",
    "    temp_train['BernoulliNB_1_1'] = berNB_m1.predict_proba(X_train)[:, 1]\n",
    "    temp_train['RidgeClassifier'] = ridgeC.predict(X_train)\n",
    "    temp_train['AdaBoostClassifier_0'] = ada.predict_proba(X_train)[:, 0]\n",
    "    temp_train['AdaBoostClassifier_1'] = ada.predict_proba(X_train)[:, 1]\n",
    "    temp_train['BernoulliNB_2_0'] = berNB_m2.predict_proba(X_train)[:, 0]\n",
    "    temp_train['BernoulliNB_2_1'] = berNB_m2.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "\n",
    "    temp_dev = pd.DataFrame(berNB_m1.predict_proba(X_dev)[:, 0], columns = ['BernoulliNB_1_0'])\n",
    "    temp_dev['BernoulliNB_1_1'] = berNB_m1.predict_proba(X_dev)[:, 1]\n",
    "    temp_dev['RidgeClassifier'] = ridgeC.predict(X_dev)\n",
    "    temp_dev['AdaBoostClassifier_0'] = ada.predict_proba(X_dev)[:, 0]\n",
    "    temp_dev['AdaBoostClassifier_1'] = ada.predict_proba(X_dev)[:, 1]\n",
    "    temp_dev['BernoulliNB_2_0'] = berNB_m2.predict_proba(X_dev)[:, 0]\n",
    "    temp_dev['BernoulliNB_2_1'] = berNB_m2.predict_proba(X_dev)[:, 1]\n",
    "    \n",
    "\n",
    "    temp_test = pd.DataFrame(berNB_m1.predict_proba(X_test)[:, 0], columns = ['BernoulliNB_1_0'])\n",
    "    temp_test['BernoulliNB_1_1'] = berNB_m1.predict_proba(X_test)[:, 1]\n",
    "    temp_test['RidgeClassifier'] = ridgeC.predict(X_test)\n",
    "    temp_test['AdaBoostClassifier_0'] = ada.predict_proba(X_test)[:, 0]\n",
    "    temp_test['AdaBoostClassifier_1'] = ada.predict_proba(X_test)[:, 1]\n",
    "    temp_test['BernoulliNB_2_0'] = berNB_m2.predict_proba(X_test)[:, 0]\n",
    "    temp_test['BernoulliNB_2_1'] = berNB_m2.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print('CatBoost...')\n",
    "\n",
    "    X_temp = [(temp_train, y_train), (temp_dev, y_dev), (temp_test, y_test)]   \n",
    "        \n",
    "    cat = CatBoostClassifier(iterations = cat_parameters['iterations'], learning_rate = cat_parameters['learning_rate'], \n",
    "                             depth = cat_parameters['depth'], logging_level = 'Verbose', loss_function='Logloss', \n",
    "                             scale_pos_weight = cat_parameters['scale_pos_weight'], random_seed = 7)\n",
    "    \n",
    "    cat.fit(temp_train, y_train)\n",
    "    get_results('CatBoostClassifier', cat, X_temp)\n",
    "    \n",
    "    del feature_tiff, y, X_train, X_dev, X_test, y_train, y_dev, y_test, temp_train, temp_dev, temp_test\n",
    "    gc.collect()\n",
    "    \n",
    "    results = {'BernoulliNB_1': berNB_m1, 'RidgeClassifier': ridgeC, \n",
    "               'AdaBoostClassifier': ada, 'BernoulliNB_2': berNB_m2, \n",
    "               'CatBoostClassifier': cat, 'tiff': tiff}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model_name, model, X):\n",
    "    '''Evaluate the performance of the models using a reference as optimum\n",
    "    model_name: name of the model\n",
    "    model: trained model\n",
    "    X: contains the train, dev and test sets to evaluate the performance\n",
    "    '''\n",
    "    optimum_aprox = .98\n",
    "    train = roc_auc_score(X[0][1], model.predict(X[0][0]))\n",
    "    dev = roc_auc_score(X[1][1], model.predict(X[1][0]))\n",
    "    test = roc_auc_score(X[2][1], model.predict(X[2][0]))\n",
    "    print('Results: ', model_name)\n",
    "    print('Optimum Bayes: ', optimum_aprox)\n",
    "    print('Auc Roc - Train: [{}], and the difference with opt_aprox is: {}'. format(train, optimum_aprox - train))\n",
    "    print('Auc Roc - Dev: [{}], and the difference with opt_aprox is: {}'. format(train, optimum_aprox - dev))\n",
    "    print('Auc Roc - Test: [{}], and the difference with opt_aprox is: {}'. format(train, optimum_aprox - test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_train_data('toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.comment_text.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pre_processing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i ca not make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congratulations from me as well use the tools...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  explanation why the edits made under my userna...      0\n",
       "1  d aww he matches this background colour i m se...      0\n",
       "2  hey man i m really not trying to edit war it s...      0\n",
       "3   more i ca not make any real suggestions on im...      0\n",
       "4  you sir are my hero any chance you remember wh...      0\n",
       "5   congratulations from me as well use the tools...      0\n",
       "6       cocksucker before you piss around on my work      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/test.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.comment_text.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pre_processing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule is more succesful then you wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>from rfc the title is fine as it is imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>sources zawe ashton on lapland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  yo bitch ja rule is more succesful then you wi...\n",
       "1  0000247867823ef7           from rfc the title is fine as it is imo \n",
       "2  00013b17ad220c46                    sources zawe ashton on lapland "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Train Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   toxic   count\n",
      "0      0  144277\n",
      "1      1   15294\n",
      "Neg examples 144277 / Pos examples 15294 = 9.433568719759382\n"
     ]
    }
   ],
   "source": [
    "ratio = get_imb_data_info('toxic') # get the ratio to use in scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost hyperparameters\n",
    "cat_parameters = {\n",
    "    'iterations': 500,\n",
    "    'depth': 7,\n",
    "    'learning_rate': .7,\n",
    "    'scale_pos_weight': ratio\n",
    "}\n",
    "models = {} # dictionary used for store the models\n",
    "\n",
    "toxic = train('toxic', random_seed = 1, cat_parameters = cat_parameters)\n",
    "models['toxic'] = toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BernoulliNB [1]...__\n",
    "\n",
    "__Results:  BernoulliNB_1__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.783624593680512], and the difference with opt_aprox is: 0.19637540631948802\n",
    "\n",
    "Auc Roc - Dev: [0.783624593680512], and the difference with opt_aprox is: 0.20484157471905318\n",
    "\n",
    "Auc Roc - Test: [0.783624593680512], and the difference with opt_aprox is: 0.20020999240473902\n",
    "\n",
    "__RidgeClassifier...__\n",
    "\n",
    "__Results:  RidgeClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.8998701469043446], and the difference with opt_aprox is: 0.08012985309565535\n",
    "\n",
    "Auc Roc - Dev: [0.8998701469043446], and the difference with opt_aprox is: 0.08799057775143693\n",
    "\n",
    "Auc Roc - Test: [0.8998701469043446], and the difference with opt_aprox is: 0.08549847091696727\n",
    "\n",
    "\n",
    "__Ada Boost...__\n",
    "\n",
    "__Results:  AdaBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.758534551363211], and the difference with opt_aprox is: 0.22146544863678896\n",
    "\n",
    "Auc Roc - Dev: [0.758534551363211], and the difference with opt_aprox is: 0.21990485741382293\n",
    "\n",
    "Auc Roc - Test: [0.758534551363211], and the difference with opt_aprox is: 0.22258360285753265\n",
    "\n",
    "__BernoulliNB [2]...__\n",
    "\n",
    "__Results:  BernoulliNB_2__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.7824438206250637], and the difference with opt_aprox is: 0.1975561793749363\n",
    "\n",
    "Auc Roc - Dev: [0.7824438206250637], and the difference with opt_aprox is: 0.20714976215561998\n",
    "\n",
    "Auc Roc - Test: [0.7824438206250637], and the difference with opt_aprox is: 0.20092315089802049\n",
    "\n",
    "Creating temp train/dev/test data...\n",
    "\n",
    "__Results:  CatBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9035294618775186], and the difference with opt_aprox is: 0.07647053812248139\n",
    "\n",
    "Auc Roc - Dev: [0.9035294618775186], and the difference with opt_aprox is: 0.08843866349513951\n",
    "\n",
    "Auc Roc - Test: [0.9035294618775186], and the difference with opt_aprox is: 0.08457861207747175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Severe Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   severe_toxic   count\n",
      "0             0  157976\n",
      "1             1    1595\n",
      "Neg examples 157976 / Pos examples 1595 = 99.04451410658307\n"
     ]
    }
   ],
   "source": [
    "ratio = get_imb_data_info('severe_toxic') # get the ratio to use in scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost hyperparameters\n",
    "cat_parameters = {\n",
    "    'iterations': 150,\n",
    "    'depth': 10,\n",
    "    'learning_rate': 1.7, \n",
    "    'scale_pos_weight': ratio\n",
    "}\n",
    "severe_toxic = train('severe_toxic', random_seed = 2, cat_parameters = cat_parameters)\n",
    "models['severe_toxic'] = severe_toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BernoulliNB [1]...__\n",
    "\n",
    "__Results:  BernoulliNB_1__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9217749301137986], and the difference with opt_aprox is: 0.05822506988620135\n",
    "\n",
    "Auc Roc - Dev: [0.9217749301137986], and the difference with opt_aprox is: 0.06328550528921906\n",
    "\n",
    "Auc Roc - Test: [0.9217749301137986], and the difference with opt_aprox is: 0.06386731509011156\n",
    "\n",
    "__RidgeClassifier...__\n",
    "\n",
    "__Results:  RidgeClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9336104423680593], and the difference with opt_aprox is: 0.04638955763194064\n",
    "\n",
    "Auc Roc - Dev: [0.9336104423680593], and the difference with opt_aprox is: 0.051252532072923684\n",
    "\n",
    "Auc Roc - Test: [0.9336104423680593], and the difference with opt_aprox is: 0.0747108040969825\n",
    "\n",
    "__Ada Boost...__\n",
    "\n",
    "__Results:  AdaBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.6453256683857083], and the difference with opt_aprox is: 0.3346743316142917\n",
    "\n",
    "Auc Roc - Dev: [0.6453256683857083], and the difference with opt_aprox is: 0.3418304073823992\n",
    "\n",
    "Auc Roc - Test: [0.6453256683857083], and the difference with opt_aprox is: 0.3603490652472119\n",
    "\n",
    "__BernoulliNB [2]...__\n",
    "\n",
    "__Results:  BernoulliNB_2__\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.8982335338249797], and the difference with opt_aprox is: 0.08176646617502026\n",
    "\n",
    "Auc Roc - Dev: [0.8982335338249797], and the difference with opt_aprox is: 0.0945397254107585\n",
    "\n",
    "Auc Roc - Test: [0.8982335338249797], and the difference with opt_aprox is: 0.08929247439297694\n",
    "Creating temp train/dev/test data...\n",
    "\n",
    "__CatBoost...__\n",
    "\n",
    "__Results:  CatBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9614800355768387], and the difference with opt_aprox is: 0.018519964423161284\n",
    "\n",
    "Auc Roc - Dev: [0.9614800355768387], and the difference with opt_aprox is: 0.038912896691424614\n",
    "\n",
    "Auc Roc - Test: [0.9614800355768387], and the difference with opt_aprox is: 0.04633459772726911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   obscene   count\n",
      "0        0  151122\n",
      "1        1    8449\n",
      "Neg examples 151122 / Pos examples 8449 = 17.886377086045687\n"
     ]
    }
   ],
   "source": [
    "ratio = get_imb_data_info('obscene') # get the ratio to use in scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost hyperparameters\n",
    "cat_parameters = {\n",
    "    'iterations': 500,\n",
    "    'depth': 7,\n",
    "    'learning_rate': .7,\n",
    "    'scale_pos_weight': ratio\n",
    "}\n",
    "obscene = train('obscene', random_seed = 3, cat_parameters = cat_parameters)\n",
    "models['obscene'] = obscene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BernoulliNB [1]...__\n",
    "\n",
    "__Results:  BernoulliNB_1__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.8198153045186166], and the difference with opt_aprox is: 0.16018469548138337\n",
    "\n",
    "Auc Roc - Dev: [0.8198153045186166], and the difference with opt_aprox is: 0.16932531246538285\n",
    "\n",
    "Auc Roc - Test: [0.8198153045186166], and the difference with opt_aprox is: 0.16701625949202092\n",
    "\n",
    "__RidgeClassifier...__\n",
    "\n",
    "__Results:  RidgeClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.944500391927917], and the difference with opt_aprox is: 0.03549960807208297\n",
    "\n",
    "Auc Roc - Dev: [0.944500391927917], and the difference with opt_aprox is: 0.04127218903619889\n",
    "\n",
    "Auc Roc - Test: [0.944500391927917], and the difference with opt_aprox is: 0.036747889908589015\n",
    "\n",
    "__Ada Boost...__\n",
    "\n",
    "__Results:  AdaBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.7812237771979266], and the difference with opt_aprox is: 0.1987762228020734\n",
    "\n",
    "Auc Roc - Dev: [0.7812237771979266], and the difference with opt_aprox is: 0.20161968761533233\n",
    "\n",
    "Auc Roc - Test: [0.7812237771979266], and the difference with opt_aprox is: 0.2071881884947796\n",
    "\n",
    "__BernoulliNB [2]...__\n",
    "\n",
    "__Results:  BernoulliNB_2__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.8166738071357879], and the difference with opt_aprox is: 0.16332619286421213\n",
    "\n",
    "Auc Roc - Dev: [0.8166738071357879], and the difference with opt_aprox is: 0.17464152414860745\n",
    "\n",
    "Auc Roc - Test: [0.8166738071357879], and the difference with opt_aprox is: 0.1704672134997386\n",
    "\n",
    "Creating temp train/dev/test data...\n",
    "\n",
    "__CatBoost...__\n",
    "\n",
    "__Results:  CatBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9491419169077011], and the difference with opt_aprox is: 0.03085808309229887\n",
    "\n",
    "Auc Roc - Dev: [0.9491419169077011], and the difference with opt_aprox is: 0.043009306188520724\n",
    "\n",
    "Auc Roc - Test: [0.9491419169077011], and the difference with opt_aprox is: 0.038944005357914535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   threat   count\n",
      "0       0  159093\n",
      "1       1     478\n",
      "Neg examples 159093 / Pos examples 478 = 332.8305439330544\n"
     ]
    }
   ],
   "source": [
    "ratio = get_imb_data_info('threat') # get the ratio to use in scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost hyperparameters\n",
    "cat_parameters = {\n",
    "    'iterations': 500,\n",
    "    'depth': 7,\n",
    "    'learning_rate': .7,\n",
    "    'scale_pos_weight': ratio\n",
    "}\n",
    "threat = train('threat', random_seed = 4, cat_parameters = cat_parameters)\n",
    "models['threat'] = threat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BernoulliNB [1]...__\n",
    "\n",
    "__Results:  BernoulliNB_1__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.7184614310366918], and the difference with opt_aprox is: 0.2615385689633082\n",
    "\n",
    "Auc Roc - Dev: [0.7184614310366918], and the difference with opt_aprox is: 0.2836815100383655\n",
    "\n",
    "Auc Roc - Test: [0.7184614310366918], and the difference with opt_aprox is: 0.2741168485878607\n",
    "\n",
    "__RidgeClassifier...__\n",
    "\n",
    "__Results:  RidgeClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.7818490339867059], and the difference with opt_aprox is: 0.19815096601329407\n",
    "\n",
    "Auc Roc - Dev: [0.7818490339867059], and the difference with opt_aprox is: 0.22450124781167358\n",
    "\n",
    "Auc Roc - Test: [0.7818490339867059], and the difference with opt_aprox is: 0.17908965631206042\n",
    "\n",
    "__Ada Boost...__\n",
    "\n",
    "__Results:  AdaBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.616076687072164], and the difference with opt_aprox is: 0.363923312927836\n",
    "\n",
    "Auc Roc - Dev: [0.616076687072164], and the difference with opt_aprox is: 0.3970228517152754\n",
    "\n",
    "Auc Roc - Test: [0.616076687072164], and the difference with opt_aprox is: 0.3889535326351572\n",
    "\n",
    "__BernoulliNB [2]...__\n",
    "\n",
    "__Results:  BernoulliNB_2__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9030588812698119], and the difference with opt_aprox is: 0.07694111873018805\n",
    "\n",
    "Auc Roc - Dev: [0.9030588812698119], and the difference with opt_aprox is: 0.10252998472827513\n",
    "\n",
    "Auc Roc - Test: [0.9030588812698119], and the difference with opt_aprox is: 0.14641082615533973\n",
    "\n",
    "Creating temp train/dev/test data...\n",
    "\n",
    "__CatBoost...__\n",
    "\n",
    "__Results:  CatBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9944820588340937], and the difference with opt_aprox is: -0.014482058834093703\n",
    "\n",
    "Auc Roc - Dev: [0.9944820588340937], and the difference with opt_aprox is: 0.19384372555592788\n",
    "\n",
    "Auc Roc - Test: [0.9944820588340937], and the difference with opt_aprox is: 0.17619573717182635\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   insult   count\n",
      "0       0  151694\n",
      "1       1    7877\n",
      "Neg examples 151694 / Pos examples 7877 = 19.25783927891329\n"
     ]
    }
   ],
   "source": [
    "ratio = get_imb_data_info('insult') # get the ratio to use in scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost hyperparameters\n",
    "cat_parameters = {\n",
    "    'iterations': 500,\n",
    "    'depth': 7,\n",
    "    'learning_rate': .7,\n",
    "    'scale_pos_weight': ratio\n",
    "}\n",
    "insult = train('insult', random_seed = 5, cat_parameters = cat_parameters)\n",
    "models['insult'] = insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BernoulliNB [1]...__\n",
    "\n",
    "__Results:  BernoulliNB_1__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.8201858624078954], and the difference with opt_aprox is: 0.1598141375921046\n",
    "\n",
    "Auc Roc - Dev: [0.8201858624078954], and the difference with opt_aprox is: 0.17470001382394762\n",
    "\n",
    "Auc Roc - Test: [0.8201858624078954], and the difference with opt_aprox is: 0.1743627323032918\n",
    "\n",
    "__RidgeClassifier...__\n",
    "\n",
    "__Results:  RidgeClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9224391131067932], and the difference with opt_aprox is: 0.05756088689320682\n",
    "\n",
    "Auc Roc - Dev: [0.9224391131067932], and the difference with opt_aprox is: 0.06476813072133292\n",
    "\n",
    "Auc Roc - Test: [0.9224391131067932], and the difference with opt_aprox is: 0.06339873527006346\n",
    "\n",
    "__Ada Boost...__\n",
    "\n",
    "__Results:  AdaBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.6946106695855874], and the difference with opt_aprox is: 0.2853893304144126\n",
    "\n",
    "Auc Roc - Dev: [0.6946106695855874], and the difference with opt_aprox is: 0.28666040774877855\n",
    "\n",
    "Auc Roc - Test: [0.6946106695855874], and the difference with opt_aprox is: 0.2887656293525379\n",
    "\n",
    "__BernoulliNB [2]...__\n",
    "\n",
    "__Results:  BernoulliNB_2__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.817788825352185], and the difference with opt_aprox is: 0.16221117464781498\n",
    "\n",
    "Auc Roc - Dev: [0.817788825352185], and the difference with opt_aprox is: 0.1792456738765158\n",
    "\n",
    "Auc Roc - Test: [0.817788825352185], and the difference with opt_aprox is: 0.1777028415598484\n",
    "\n",
    "Creating temp train/dev/test data...\n",
    "\n",
    "__CatBoost...__\n",
    "\n",
    "__Results:  CatBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9315620903690014], and the difference with opt_aprox is: 0.04843790963099859\n",
    "\n",
    "Auc Roc - Dev: [0.9315620903690014], and the difference with opt_aprox is: 0.061975395605845085\n",
    "\n",
    "Auc Roc - Test: [0.9315620903690014], and the difference with opt_aprox is: 0.0663485522413122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Identity Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   identity_hate   count\n",
      "0              0  158166\n",
      "1              1    1405\n",
      "Neg examples 158166 / Pos examples 1405 = 112.57366548042705\n"
     ]
    }
   ],
   "source": [
    "ratio = get_imb_data_info('identity_hate') # get the ratio to use in scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost hyperparameters\n",
    "cat_parameters = {\n",
    "    'iterations': 500,\n",
    "    'depth': 7,\n",
    "    'learning_rate': .7,\n",
    "    'scale_pos_weight': ratio\n",
    "}\n",
    "identity_hate = train('identity_hate', random_seed = 6, cat_parameters = cat_parameters)\n",
    "models['identity_hate'] = identity_hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BernoulliNB [1]...__\n",
    "\n",
    "__Results:  BernoulliNB_1__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9177071443264864], and the difference with opt_aprox is: 0.062292855673513614\n",
    "\n",
    "Auc Roc - Dev: [0.9177071443264864], and the difference with opt_aprox is: 0.09336792532923155\n",
    "\n",
    "Auc Roc - Test: [0.9177071443264864], and the difference with opt_aprox is: 0.08310948416642094\n",
    "\n",
    "__RidgeClassifier...__\n",
    "\n",
    "__Results:  RidgeClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.8609598341059914], and the difference with opt_aprox is: 0.11904016589400856\n",
    "\n",
    "Auc Roc - Dev: [0.8609598341059914], and the difference with opt_aprox is: 0.13326544778989113\n",
    "\n",
    "Auc Roc - Test: [0.8609598341059914], and the difference with opt_aprox is: 0.14272304596007768\n",
    "\n",
    "__Ada Boost...__\n",
    "\n",
    "__Results:  AdaBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.6190787711703019], and the difference with opt_aprox is: 0.3609212288296981\n",
    "\n",
    "Auc Roc - Dev: [0.6190787711703019], and the difference with opt_aprox is: 0.3529441317625437\n",
    "\n",
    "Auc Roc - Test: [0.6190787711703019], and the difference with opt_aprox is: 0.35546132371815675\n",
    "\n",
    "__BernoulliNB [2]...__\n",
    "\n",
    "__Results:  BernoulliNB_2__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9211667120630912], and the difference with opt_aprox is: 0.058833287936908785\n",
    "\n",
    "Auc Roc - Dev: [0.9211667120630912], and the difference with opt_aprox is: 0.08835781241416496\n",
    "\n",
    "Auc Roc - Test: [0.9211667120630912], and the difference with opt_aprox is: 0.09327503633158363\n",
    "\n",
    "Creating temp train/dev/test data...\n",
    "\n",
    "__CatBoost...__\n",
    "\n",
    "__Results:  CatBoostClassifier__\n",
    "\n",
    "Optimum Bayes:  0.98\n",
    "\n",
    "Auc Roc - Train: [0.9599980026430752], and the difference with opt_aprox is: 0.02000199735692476\n",
    "\n",
    "Auc Roc - Dev: [0.9599980026430752], and the difference with opt_aprox is: 0.10260798345976563\n",
    "\n",
    "Auc Roc - Test: [0.9599980026430752], and the difference with opt_aprox is: 0.11225525497031041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Pre processing...\n",
      "Predictions\n"
     ]
    }
   ],
   "source": [
    "submit = predict(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = read_test_data()\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.996660</td>\n",
       "      <td>0.998809</td>\n",
       "      <td>0.995900</td>\n",
       "      <td>9.888269e-01</td>\n",
       "      <td>0.985003</td>\n",
       "      <td>0.023413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.151661</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.032648</td>\n",
       "      <td>1.489659e-07</td>\n",
       "      <td>0.077905</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.302894</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>1.489659e-07</td>\n",
       "      <td>0.178522</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.092115</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.016664</td>\n",
       "      <td>1.750237e-11</td>\n",
       "      <td>0.017861</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
       "0  00001cee341fdb12  0.996660      0.998809  0.995900  9.888269e-01  0.985003   \n",
       "1  0000247867823ef7  0.151661      0.000887  0.032648  1.489659e-07  0.077905   \n",
       "2  00013b17ad220c46  0.302894      0.000887  0.016752  1.489659e-07  0.178522   \n",
       "3  00017563c3f7919a  0.092115      0.000120  0.016664  1.750237e-11  0.017861   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.023413  \n",
       "1       0.000041  \n",
       "2       0.000407  \n",
       "3       0.000004  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../submits/results_preds_cat.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final score on private leaderboard: __0.9566__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
